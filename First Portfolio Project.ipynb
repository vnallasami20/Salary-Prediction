{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salary Predictions Based on Job Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - DEFINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 1 Define the problem ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Given the sample training data, train a model to predict the salary for new postings based job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import your libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import string\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'large',\n",
    "         'axes.titlesize':'large',\n",
    "         'axes.labelcolor':'#5F4C0B',\n",
    "         'axes.titlecolor':'#B40404',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large',\n",
    "         'xtick.color':'red', \n",
    "         'ytick.color':'green'}\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "from scipy import stats\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score,KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#your info here\n",
    "__author__ = \"Vijayan Nallasami\"\n",
    "__email__ = \"VNallasami@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.max_rows',500)\n",
    "np.set_printoptions(linewidth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - DISCOVER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 2 Load the data ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fileload_:\n",
    "    \n",
    "    def __init__(self,path,files,target,Id):\n",
    "        \n",
    "        self.file_path = path\n",
    "        self.file_dict = files\n",
    "        self.target_column = target\n",
    "        self.merge_Id = Id\n",
    "        self.load_data()\n",
    "        self.merge_df()\n",
    "        self.find_feature_type()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \n",
    "        for name, file in self.file_dict.items():\n",
    "            \n",
    "            if name == 'train_feature':\n",
    "                self.train_features = pd.read_csv(path+file)\n",
    "            if name == 'train_target':\n",
    "                self.train_target = pd.read_csv(path+file)\n",
    "            if name == 'test':\n",
    "                self.test_features = pd.read_csv(path+file)\n",
    "                \n",
    "    def merge_df(self):\n",
    "        \n",
    "        self.train = pd.merge(self.train_features,self.train_target,on=self.merge_Id)\n",
    "        self.train.drop(labels=self.merge_Id,axis=1,inplace=True)\n",
    "        \n",
    "    def find_feature_type(self):\n",
    "        \n",
    "        self.numerical_features = self.train.select_dtypes(exclude='object').columns\n",
    "        self.categorical_features = self.train.select_dtypes('object').columns            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 3 Clean the data ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datacleaning_:\n",
    "    \n",
    "    def __init__(self,dataframe):\n",
    "        self.train_df = dataframe.train\n",
    "        self.target_column = dataframe.target_column\n",
    "        self.remove_dupes()\n",
    "        self.remove_invalid_target()\n",
    "        dataframe.train = self.train_df        \n",
    "    \n",
    "    def remove_dupes(self):\n",
    "        \n",
    "        print(\"Duplicate rows : \",len(self.train_df[self.train_df.duplicated(keep='first')]))\n",
    "        self.train_df.drop_duplicates(keep='first', inplace=True)\n",
    "        \n",
    "    def remove_invalid_target(self):\n",
    "        \n",
    "        dupes = self.train_df[self.train_df[self.target_column] < 1].index\n",
    "        self.train_df.drop(dupes,inplace=True)       \n",
    "        print(\"No.of rows with invalid target\",len(dupes))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_ordered_mean_encoding(X,y,target):\n",
    "    \n",
    "    X_feature = X.copy()\n",
    "    y_target = y.copy()\n",
    "    \n",
    "    df = pd.concat([X_feature.copy(),y_target.copy()],axis=1)\n",
    "    \n",
    "    categorical = X.select_dtypes('O').columns\n",
    "    \n",
    "    for col in categorical:\n",
    "        \n",
    "        col_mean = df.groupby([col])[target].mean().sort_values(ascending=True).index\n",
    "        col_dict = {val: key for key, val in enumerate(col_mean,1)}\n",
    "        \n",
    "        df[col + '_enc'] = df[col].map(col_dict)\n",
    "        \n",
    "    return df.drop(labels=target,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_combined_mean_encoding(X,y,target,var_dict):\n",
    "    \n",
    "    X_feature = X.copy()\n",
    "    y_target = y.copy()\n",
    "    \n",
    "    df = pd.concat([X_feature.copy(),y_target.copy()],axis=1)\n",
    "        \n",
    "    for key in var_dict:\n",
    "        \n",
    "        df[key] = df[var_dict[key][0]] + ' ' + df[var_dict[key][1]]\n",
    "        \n",
    "        col_mean = df.groupby(key)[target].mean().sort_values(ascending=True).index\n",
    "        col_dict = {val: key for key, val in enumerate(col_mean,1)}\n",
    "\n",
    "        df[key] = df[key].map(col_dict)\n",
    "        \n",
    "    return df.drop(labels=target,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_feature(df):\n",
    "    dup_list = set()\n",
    "    for i in range(0,len(df.columns)):\n",
    "        col1 = df.columns[i]\n",
    "        for col2 in df.columns[i+1:]:\n",
    "            if df[col1].equals(df[col2]):\n",
    "#                 print(col1,col2)\n",
    "                dup_list.add(col2)\n",
    "                \n",
    "    df.drop(labels=dup_list,axis=1,inplace=True)\n",
    "    return df,dup_list\n",
    "\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 4 Explore the data (EDA) ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarize each feature variable\n",
    "#summarize the target variable\n",
    "#look for correlation between each feature and the target\n",
    "#look for correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class data_analysis_:\n",
    "    \n",
    "    def __init__(self):        \n",
    "        self.dataframe = dataframe.train\n",
    "        self.target = dataframe.target_column\n",
    "        self.numerical_features = dataframe.numerical_features\n",
    "        self.categorical_features = dataframe.categorical_features\n",
    "#         self.cardinality_plot()\n",
    "        \n",
    "    def explore_data(self, pred=None): \n",
    "        obs = self.dataframe.shape[0]\n",
    "        types = self.dataframe.dtypes\n",
    "        counts = self.dataframe.apply(lambda x: x.count())\n",
    "        uniques = self.dataframe.apply(lambda x: [x.unique()])\n",
    "        nulls = self.dataframe.apply(lambda x: x.isnull().sum())\n",
    "        distincts = self.dataframe.apply(lambda x: x.unique().shape[0])\n",
    "        missing_ratio = (self.dataframe.isnull().sum()/ obs) * 100\n",
    "        skewness = self.dataframe.skew()\n",
    "        kurtosis = self.dataframe.kurt() \n",
    "        \n",
    "        if pred is None:\n",
    "            cols = ['Types', 'Count', 'DistinctValues', 'Nulls', 'MissingRatio', 'Skewness', 'Kurtosis']\n",
    "            self.eda = pd.concat([types, counts, distincts, nulls, missing_ratio,  skewness, kurtosis], axis = 1)\n",
    "\n",
    "        else:\n",
    "            corr = self.dataframe.corr()[pred]\n",
    "            self.eda = pd.concat([types, counts, distincts, nulls, missing_ratio,  skewness, kurtosis, corr], axis = 1, sort=False)\n",
    "            corr_col = 'corr '  + pred\n",
    "            cols = ['Types', 'Count', 'DistinctValues', 'Nulls', 'MissingRatio',  'Skewness', 'Kurtosis', corr_col ]\n",
    "        self.eda.columns = cols\n",
    "        \n",
    "        print(self.eda)\n",
    "        \n",
    "        for index, value in uniques.items():\n",
    "            if len(value[0]) < 20:\n",
    "                print('-'*120)\n",
    "                print(index, ' : ',value[0])\n",
    "            else:\n",
    "                print('-'*120)\n",
    "                print(index,' : {} unique values'.format(len(value[0])))\n",
    "        print('-'*120)\n",
    "                \n",
    "    def cardinality_plot(self,h,w):\n",
    "    \n",
    "        sns.set(rc={'figure.figsize':(w,h)})\n",
    "        df = self.dataframe[self.categorical_features].nunique().to_frame().reset_index()\n",
    "        df.columns = ['column','count']\n",
    "        b = sns.barplot(x=\"column\", y=\"count\", data=df)\n",
    "        b.axes.set_title(\"Categorical columns cardinality\",fontsize=20)\n",
    "        b.set_xlabel(\"Column Name\",fontsize=15)\n",
    "        b.set_ylabel(\"Count of Categories\",fontsize=15)\n",
    "        b.tick_params(labelsize=15)\n",
    "        plt.show()\n",
    "                \n",
    "    def displot(self,col):\n",
    "        \n",
    "        mean = dataframe.train[[col]].mean()\n",
    "        b = sns.distplot(dataframe.train[[col]],color='red')\n",
    "        \n",
    "        title = col + \" Distribution\"\n",
    "        \n",
    "        b.axes.set_title(title.title(),fontsize=20)\n",
    "        plt.axvline(mean[0],0,1,color='blue')\n",
    "    \n",
    "    def linegraph(self,var_list,fig_h=None,fig_w=None):\n",
    "        \n",
    "        nrow, ncol = choose_subplot_dimensions(len(var_list))\n",
    "        axis_arr = generate_axis_array(nrow,ncol)\n",
    "        fig, axes = plt.subplots(nrow, ncol, figsize=(fig_h, fig_w))        \n",
    "        \n",
    "        for ind,col in enumerate(var_list):  \n",
    "            \n",
    "            df = self.dataframe.groupby([col])[self.target].mean().sort_values()\n",
    "            df = df.to_frame().reset_index()  \n",
    "\n",
    "            ax=axes[axis_arr[ind][0],axis_arr[ind][1]]\n",
    "            ax.plot(df[self.target],df[col])            \n",
    "            \n",
    "            ax.set_xlabel(self.target)\n",
    "            ax.set_title(string.capwords(col))\n",
    "            \n",
    "        if (len(var_list) % 2) != 0:\n",
    "            print(nrow,ncol,nrow+ncol % 2)\n",
    "            fig.delaxes(axes[axis_arr[ind+1][0],axis_arr[ind+1][1]])    \n",
    "            \n",
    "        plt.tight_layout()        \n",
    "        \n",
    "        \n",
    "    def barplot(self,var_list,fig_h=None,fig_w=None):\n",
    "        \n",
    "        nrow, ncol = choose_subplot_dimensions(len(var_list))\n",
    "        axis_arr = generate_axis_array(nrow,ncol)\n",
    "        fig, axes = plt.subplots(nrow, ncol, figsize=(fig_h, fig_w))        \n",
    "        \n",
    "        for ind,col in enumerate(var_list):  \n",
    "            \n",
    "            df = dataframe.train.groupby([col])[self.target].count().to_frame().reset_index()\n",
    "            df.columns = [col,'count']\n",
    "            \n",
    "            ax=axes[axis_arr[ind][0],axis_arr[ind][1]]\n",
    "            sns.barplot(x=\"count\", y=col, ci=False,  data=df,ax=ax)          \n",
    "            \n",
    "#             ax.set_xlabel(self.target)\n",
    "#             ax.set_title(string.capwords(col))\n",
    "            \n",
    "        if (len(var_list) % 2) != 0:\n",
    "            print(nrow,ncol,nrow+ncol % 2)\n",
    "            fig.delaxes(axes[axis_arr[ind+1][0],axis_arr[ind+1][1]])    \n",
    "            \n",
    "        plt.tight_layout()\n",
    "        \n",
    "    \n",
    "    def barplothue(self,var_x,var_hue,fig_h=12,fig_w=5):\n",
    "        \n",
    "        sns.set(rc={'figure.figsize':(fig_h,fig_w)})\n",
    "        g = sns.barplot(x=var_x, y=self.target, data=self.dataframe, ci=False, hue = var_hue, orient = 'v')\n",
    "        g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n",
    "        \n",
    "\n",
    "        \n",
    "    def choose_subplot_dimensions(k):\n",
    "        if k < 4:\n",
    "            return k, 1\n",
    "        elif k < 11:\n",
    "            return math.ceil(k/2), 2\n",
    "        else:        \n",
    "            return math.ceil(k/3), 3\n",
    "\n",
    "    def generate_axis_array(nrow, ncol):\n",
    "        axis = []\n",
    "        for r in range(nrow):\n",
    "            for c in range(ncol):            \n",
    "                axis.append([r,c])\n",
    "        return axis          \n",
    "                        \n",
    "    def correlation(self, threshold):\n",
    "        col_corr = set()  # Set of all the names of correlated columns\n",
    "        corr_matrix = self.dataframe.corr()\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                    colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                    col_corr.add(colname)\n",
    "        return col_corr    \n",
    "    \n",
    "\n",
    "    def diagnostic_plots(self, variable,col=None):\n",
    "        # function takes a dataframe (df) and\n",
    "        # the variable of interest as arguments\n",
    "\n",
    "        # define figure size\n",
    "        plt.figure(figsize=(16, 4))\n",
    "\n",
    "        # histogram\n",
    "        plt.subplot(1, 3, 1)\n",
    "        sns.distplot(self.dataframe[variable], bins=30,color=col,kde_kws={'bw':0.1})\n",
    "        plt.title('Histogram')\n",
    "\n",
    "        # Q-Q plot\n",
    "        plt.subplot(1, 3, 2)\n",
    "        stats.probplot(self.dataframe[variable].astype(float), dist=\"norm\", plot=plt)\n",
    "        plt.ylabel('Variable quantiles')\n",
    "\n",
    "        # boxplot\n",
    "        plt.subplot(1, 3, 3)\n",
    "        sns.boxplot(y=self.dataframe[variable].astype(float),color=col)\n",
    "        plt.title('Boxplot')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "filenames = {'train_feature':'train_features.csv','train_target':'train_salaries.csv','test':'test_features.csv'}\n",
    "target = 'salary'\n",
    "Id = 'jobId'\n",
    "\n",
    "dataframe = fileload_(path,filenames,target,Id)\n",
    "data_clean = datacleaning_(dataframe)\n",
    "dataframe.train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = data_analysis_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "da.explore_data('salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "da.cardinality_plot(5,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.displot('salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.diagnostic_plots('salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.linegraph(['jobType', 'degree', 'major', 'industry','yearsExperience','milesFromMetropolis'],15,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.barplot(['jobType', 'degree', 'major', 'industry','yearsExperience','milesFromMetropolis'],15,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.barplothue('degree','major')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.barplothue('degree','jobType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.barplothue('jobType','industry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.barplothue('degree','industry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"degree_major\", y=\"salary\", data=pd.concat([X_train,y_train],axis=1), color = '#EE67CF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataframe.train.drop('salary',axis=1)\n",
    "y_train = dataframe.train.salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cat_ordered_mean_encoding(X_train,y_train,'salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_major = {'degree_major_enc':['degree','major'],'company_industry':['companyId','industry'],'jobTypeIndustry':['jobType','industry']}\n",
    "X_train = cat_combined_mean_encoding(X_train,y_train,'salary',degree_major)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.drop(['companyId','jobType','degree','major','industry'],axis=1)\n",
    "X_train = X_train.drop(['degree_enc','major_enc','industry_enc','milesFromMetropolis'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = pd.concat([X_train,y_train],axis=1)\n",
    "\n",
    "sns.set(rc={'figure.figsize':(16,12)})\n",
    "pc = train.corr(method ='pearson')\n",
    "cols = train.columns\n",
    "ax = sns.heatmap(pc, annot=True,yticklabels=cols,xticklabels=cols,annot_kws={'size': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = EqualFrequencyDiscretiser(q=10, variables = ['degree_major_enc'])\n",
    "disc.fit(X_train)\n",
    "X_train = disc.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(X_train, 'degree_major_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(X_train, 'milesFromMetropolis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.degree_major_enc.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine import variable_transformers as vt\n",
    "lt = vt.LogTransformer(variables = ['jobTypeIndustry'])\n",
    "lt.fit(X_train)\n",
    "X_train = lt.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(X_train, 'company_industry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostic_plots(df, variable):\n",
    "    # function takes a dataframe (df) and\n",
    "    # the variable of interest as arguments\n",
    "\n",
    "    # define figure size\n",
    "    plt.figure(figsize=(16, 4))\n",
    "\n",
    "    # histogram\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.distplot(df[variable], bins=30)\n",
    "    plt.title('Histogram')\n",
    "\n",
    "    # Q-Q plot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
    "    plt.ylabel('Variable quantiles')\n",
    "\n",
    "    # boxplot\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.boxplot(y=df[variable])\n",
    "    plt.title('Boxplot')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostic_plots(df, variable):\n",
    "    \n",
    "    # function to plot a histogram and a Q-Q plot\n",
    "    # side by side, for a certain variable\n",
    "    \n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[variable].hist()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.displot('company_industry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.groupby(['jobType']).jobType_enc.nunique()\n",
    "# pd.crosstab(X_train.jobType,X_train.jobType_enc)\n",
    "# X_train.groupby('companyId').industry.unique().to_dict()\n",
    "\n",
    "X_train.groupby(['degree','major'])['degree_major_enc'].apply(lambda x: x.unique()).sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 5 Establish a baseline ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select a reasonable metric (MSE in this case)\n",
    "#create an extremely simple model and measure its efficacy\n",
    "#e.g. use \"average salary\" for each industry as your model and then measure MSE\n",
    "#during 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 6 Hypothesize solution ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brainstorm 3 models that you think may improve results over the baseline model based\n",
    "#on your "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brainstorm 3 models that you think may improve results over the baseline model based on your EDA and explain why they're reasonable solutions here.\n",
    "\n",
    "Also write down any new features that you think you should try adding to the model based on your EDA, e.g. interaction variables, summary statistics for each group, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - DEVELOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will cycle through creating features, tuning models, and training/validing models (steps 7-9) until you've reached your efficacy goal\n",
    "\n",
    "#### Your metric will be MSE and your goal is:\n",
    " - <360 for entry-level data science roles\n",
    " - <320 for senior data science roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 7 Engineer features  ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure that data is ready for modeling\n",
    "#create any new features needed to potentially enhance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_engineering:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y      \n",
    "\n",
    "        self.rare_encoding('major', 0.6)         \n",
    "        self.discretiser()\n",
    "    \n",
    "    def transform(self,X, y=None):        \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self,X, y):\n",
    "        \n",
    "        return self.fit(X,y).transform(X,y=None)\n",
    "    \n",
    "    def find_non_rare_labels(self, variable, tolerance):\n",
    "    \n",
    "        temp = self.X.groupby([variable])[variable].count() / len(self.X)\n",
    "\n",
    "        non_rare = [x for x in temp.loc[temp>tolerance].index.values]\n",
    "\n",
    "        return non_rare\n",
    "    \n",
    "    def rare_encoding(self, variable, tolerance):   \n",
    "\n",
    "        # find the most frequent category\n",
    "        frequent_cat = self.find_non_rare_labels(variable, tolerance)\n",
    "\n",
    "        # re-group rare labels\n",
    "        self.X[variable] = np.where(self.X[variable].isin(frequent_cat), self.X[variable], '2')\n",
    "\n",
    "    \n",
    "    def discretiser(self):\n",
    "        \n",
    "        disc = EqualWidthDiscretiser(bins=10, variables = ['companyId', 'milesFromMetropolis','yearsExperience'])\n",
    "        disc.fit(self.X)\n",
    "        self.X  = disc.transform(self.X)\n",
    "    \n",
    "#     def OneHotEncoding_(self):      \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove constant and quasi constants *********************************************************\n",
    "from sklearn.feature_selection import VarianceThreshold,mutual_info_regression,SelectKBest,SelectPercentile\n",
    "\n",
    "var = VarianceThreshold(threshold=0.01)\n",
    "var.fit(X_train)\n",
    "X_train = pd.DataFrame(var.transform(X_train))\n",
    "X_train.columns = columns = [ 'col'+ str(c) for c in range(0,len(X_train.columns))]\n",
    "print('remove constant and quasi constants',len(X_train.columns)-sum(var.get_support()))\n",
    "\n",
    "# remove duplicate features *********************************************************\n",
    "\n",
    "# X_train, duplicate_feature = remove_duplicate_feature(X_train)\n",
    "# print('remove duplicate features',len(duplicate_feature))\n",
    "\n",
    "# remove co-related features *********************************************************\n",
    "\n",
    "corr_features = correlation(X_train, 0.8)\n",
    "X_train.drop(labels=corr_features,axis=1,inplace=True)\n",
    "print('remove co-related features',len(corr_features),corr_features)\n",
    "\n",
    "# # mutual information features *********************************************************\n",
    "# sel = SelectKBest(mutual_info_regression,k='all')\n",
    "# sel.fit(X_train,y_train)\n",
    "# print('select K-30  best features',X_train.columns[sel.get_support()])\n",
    "# sel_features = X_train.columns[sel.get_support()].to_list()\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.fillna(0))\n",
    "sel_ = SelectFromModel(Lasso(alpha=100))\n",
    "sel_.fit(, y_train)\n",
    "\n",
    "# make a list with the selected features and print the outputs\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(np.sum(sel_.estimator_.coef_ == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_prc = []\n",
    "\n",
    "# pipe_prc.append(('num_impute',mdi.MeanMedianImputer(imputation_method='median',variables=numerical)))\n",
    "# pipe_prc.append(('cat_impute',mdi.CategoricalVariableImputer(imputation_method='missing',variables=categorical)))\n",
    "# pipe_prc.append(('rare_label',RareLabelCategoricalEncoder(tol=0.06,n_categories=2,replace_with='2',return_object=True)))\n",
    "# pipe_prc.append(('cat_enc',OrdinalCategoricalEncoder(encoding_method='ordered')))\n",
    "# pipe_prc.append(('discretiser',EqualFrequencyDiscretiser(q=10, variables=['companyId_enc','milesFromMetropolis','degree_major_enc','company_industry','jobTypeIndustry'], return_object=True)))\n",
    "# pipe_prc.append(('cat_enc',OrdinalCategoricalEncoder(encoding_method='ordered')))\n",
    "pipe_prc.append(('cat_enc',OneHotCategoricalEncoder(top_categories=None,drop_last=True)))\n",
    "pipe_prc.append(('outliers',Winsorizer(distribution='gaussian',tail='both',fold=3)))\n",
    "pipe_prc.append(('scaler',StandardScaler()))\n",
    "\n",
    "pipe = Pipeline(pipe_prc)\n",
    "pipe.fit(X_train,y_train)\n",
    "X_train = pipe.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "\n",
    "# X_train.drop('companyId',axis=1,inplace=True)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_proc = preprocessing(test,'jobId','salary')\n",
    "test_id = pre_proc.split_id()\n",
    "test = fe.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_features = correlation(X_train, 0.8)\n",
    "# train_transformed.drop(labels=corr_features,axis=1,inplace=True)\n",
    "# test_transformed.drop(labels=corr_features,axis=1,inplace=True)\n",
    "print('remove co-related features',len(corr_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE,SelectFromModel\n",
    "\n",
    "sel_ = SelectFromModel(XGBRegressor(n_estimators=100,max_depth=3,learning_rate=0.1,booster='gbtree'))\n",
    "sel_.fit(X_train, y_train)\n",
    "selected_feat = X_train.columns[(sel_.get_support())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine import missing_data_imputers as mdi\n",
    "from feature_engine.categorical_encoders import OneHotCategoricalEncoder,RareLabelCategoricalEncoder,WoERatioCategoricalEncoder\n",
    "from feature_engine.discretisers import EqualFrequencyDiscretiser, EqualWidthDiscretiser\n",
    "from feature_engine.outlier_removers import Winsorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[selected_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 8 Create models ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and tune the models that you brainstormed during part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(X,y):\n",
    "    # prepare configuration for cross validation test harness\n",
    "    seed = 7\n",
    "    # prepare models\n",
    "    models = []\n",
    "    models.append(('LR', LinearRegression()))\n",
    "    models.append(('RFR', RandomForestRegressor(n_estimators = 10, random_state = 0)))\n",
    "    #models.append(('DTR', DecisionTreeRegressor(random_state = 0)))\n",
    "    models.append(('GBR', GradientBoostingRegressor(n_estimators=40, max_depth=7, loss='ls')))\n",
    "    models.append(('XGB', XGBRegressor(n_estimators=100,max_depth=3,learning_rate=0.1)))\n",
    "#     # evaluate each model in turn\n",
    "    results = []\n",
    "    names = []\n",
    "    scoring = 'neg_mean_squared_error'\n",
    "    for name, model in models:\n",
    "        kfold = KFold(n_splits=5, random_state=seed)\n",
    "        cv_results = cross_val_score(model, X, y, cv=kfold, scoring=scoring,n_jobs=-1)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)        \n",
    "        msg = \"%s: %f (%f)\" % (name, -1.0*np.mean(cv_results), cv_results.std())\n",
    "#         msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "        print(msg)\n",
    "    # boxplot algorithm comparison\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Algorithm Comparison')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(results)\n",
    "    ax.set_xticklabels(names)\n",
    "    plt.show()\n",
    "    return names,results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 9 Test models ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do 5-fold cross validation on models and measure MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_eval(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "param_grid = {\n",
    "                'n_estimators': [100]\n",
    "              , 'max_depth': [3]\n",
    "              , 'learning_rate': [0.1]\n",
    "              ,'booster' :['gbtree']\n",
    "              ,'objective' :['reg:linear']\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(xgb, param_grid, cv=5, scoring='neg_mean_squared_error',n_jobs=-1)\n",
    "grid.fit(X_train,y_train)\n",
    "# y_pred = grid.predict(train_transformed)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean squared error :\",metrics.mean_squared_error(y_train, y_pred, squared=False)) \n",
    "print(\"r2_score :\",metrics.r2_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 10 Select best model  ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the model with the lowest error as your \"prodcuction\" model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - DEPLOY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 11 Automate pipeline ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write script that trains model on entire training set, saves model to disk,\n",
    "#and scores the \"test\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 12 Deploy solution ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save your prediction to a csv file or optionally save them as a table in a SQL database\n",
    "#additionally, you want to save a visualization and summary of your prediction and feature importances\n",
    "#these visualizations and summaries will be extremely useful to business stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---- 13 Measure efficacy ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll skip this step since we don't have the outcomes for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if isinstance(y, pd.DataFrame):\n",
    "            target = y.columns.to_list()\n",
    "        else:\n",
    "            target = y.to_frame().columns.to_list()\n",
    "            \n",
    "        self.categorical = [col for col in X.select_dtypes('O').columns]\n",
    "        \n",
    "        self.mapper = dict()\n",
    "        \n",
    "        for col in self.categorical:        \n",
    "            self.col_mean = pd.concat([X, y],axis=1).groupby(col)[target[0]].mean().sort_values(ascending=True).index\n",
    "            col_mapper = {k : n for n,k in enumerate(self.col_mean,0)}\n",
    "            self.mapper.update({col : col_mapper})\n",
    "            \n",
    "                for col in self.categorical:              \n",
    "            X[col] = X[col].map(self.mapper.get(col)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=60, n_jobs=-1, max_depth=15, min_samples_split=80, \\\n",
    "                                       max_features=8)\n",
    "\n",
    "gboost = GradientBoostingRegressor(n_estimators=40, max_depth=7, loss='ls')\n",
    "\n",
    "models = [lr, rfr, gboost]\n",
    "\n",
    "model_output = ModelEvaluation(pd.concat([X_train,y_train],axis=1), models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dict = {'JANITOR':8,'JUNIOR':7,'SENIOR':6,'MANAGER':5,'VICE_PRESIDENT':4,'CTO':3,'CFO':2,'CEO':1}\n",
    "\n",
    "train['jobType'] = train['jobType'].map(job_dict)\n",
    "\n",
    "train.jobType.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.degree.unique()\n",
    "\n",
    "degree_dict = {'DOCTORAL':1,'MASTERS':2,'BACHELORS':3,'HIGH_SCHOOL':4,'NONE':5}\n",
    "\n",
    "train.degree = train.degree.map(degree_dict)\n",
    "\n",
    "train.degree.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
